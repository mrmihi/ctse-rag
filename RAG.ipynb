{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b7353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"PINECONE_API_KEY\"] = \"pcsk_2XaLCL_AWuXRQBgwEMPrhE6BijySE9YSaX6SxGtSKeQ6Vtko2gdLqvbeRftt9bEixFDWEV\"\n",
    "# os.environ[\"PINECONE_ENVIRONMENT\"] = \"us-west1-gcp\"   # e.g. us-west1-gcp, us-east4-gcp, etc.\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBW6XKhpcRVHrthjvUBmyLHxTW7DJooWaA\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Encoder ready, dim = 384\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports & setup (assumes `pc` and `index` already created)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# instantiate the encoder\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"✅ Encoder ready, dim =\", encoder.get_sentence_embedding_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cdd62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Connect to Milvus\n",
    "from pymilvus import connections\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# adjust host/port if needed\n",
    "\n",
    "connections.connect(\n",
    "    alias=\"default\",\n",
    "    host=\"localhost\",\n",
    "    port=\"19530\"\n",
    ")\n",
    "\n",
    "# Set up embedding model\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93bede4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded and split 31 chunks from PDFs.\n"
     ]
    }
   ],
   "source": [
    "# 2) Load and split your PDFs\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "folder_path = \"./pdfs\"  # folder containing your PDFs\n",
    "loader_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "documents = []\n",
    "for fname in os.listdir(folder_path):\n",
    "    if not fname.lower().endswith(\".pdf\"):\n",
    "        continue\n",
    "    path = os.path.join(folder_path, fname)\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load_and_split(text_splitter=loader_splitter)\n",
    "    # metadata 'page' comes from loader\n",
    "    documents.extend(pages)\n",
    "\n",
    "print(f\"✅ Loaded and split {len(documents)} chunks from PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upserted documents into Milvus collection `pdf_documents`\n"
     ]
    }
   ],
   "source": [
    "# # 3) Create/overwrite Milvus collection and upsert embeddings\n",
    "# collection_name = \"pdf_documents\"\n",
    "# # create vectorstore\n",
    "# vectorstore = Milvus.from_documents(\n",
    "#     documents,\n",
    "#     embeddings,\n",
    "#     connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "#     collection_name=\"pdf_documents\",\n",
    "# )\n",
    "\n",
    "# print(f\"✅ Upserted documents into Milvus collection `{collection_name}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be07a14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upserted documents into Milvus with IVf_FLAT / COSINE (384-dim).\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# connections.connect(alias=\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# your embedding model (384-dim)\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# index + search config\n",
    "index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"params\": {\"nlist\": 128},       # number of partitions\n",
    "}\n",
    "search_params = {\n",
    "    \"metric_type\": \"COSINE\",\n",
    "    \"params\": {\"nprobe\": 10},       # how many partitions to probe at query time\n",
    "}\n",
    "\n",
    "# upsert into Milvus with IVF_FLAT/COSINE on a 384-dim vector field\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents,\n",
    "    embeddings,\n",
    "    connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "    collection_name=\"pdf_documents\",\n",
    "    index_params=index_params,\n",
    "    search_params=search_params,\n",
    "    drop_old=True,               # overwrite any existing collection\n",
    ")\n",
    "\n",
    "print(\"✅ Upserted documents into Milvus with IVf_FLAT / COSINE (384-dim).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c4ae752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain with Gemini is ready.\n"
     ]
    }
   ],
   "source": [
    "# 4) Build the RAG chain using Gemini API\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Prompt template with page-number citation\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are an expert assistant. Use the following context (with page numbers) to answer the user’s question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "1. Summary:  \n",
    "   Provide a succinct explanatory summary (1–2 sentences).\n",
    "\n",
    "2. Key Points:  \n",
    "   List the main supporting details in bullet form. For each bullet, cite the page number in parentheses.\n",
    "\n",
    "Example format:\n",
    "\n",
    "1. Summary:  \n",
    "   The primary purpose of Pinecone is to store and query dense vector embeddings for similarity search (page 12).\n",
    "\n",
    "2. Key Points:  \n",
    "   - Pinecone offers a fully managed vector database service, eliminating infrastructure overhead (page 5).  \n",
    "   - It supports cosine and dot-product similarity metrics for fast nearest-neighbor retrieval (page 8).  \n",
    "   - Integrates seamlessly with popular embedding libraries like SentenceTransformer (page 14).  \n",
    "   - Provides automatic indexing and sharding to scale to billions of vectors (page 20).\n",
    "\n",
    "Now, answer the question below following this format:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "\n",
    "# Create RetrievalQA chain\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"RAG chain with Gemini is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "11eb1e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " 1. Summary:\n",
      "Sets are unordered collections that contain only one instance of each distinct value and are similar to arrays, but unlike arrays, they must contain only one data type (page 9).\n",
      "\n",
      "2. Key Points:\n",
      "- Sets are unordered and contain only one of each distinct value (page 9).\n",
      "-  They can be created using `var evenNumbers = Set([2, 4, 6, 8])` or `var oddNumbers: Set = [1,3,5,7]` (page 9).\n",
      "-  Elements can be added using `.insert()` and removed using `.remove()` (page 9).\n",
      "- Sets are a type of collection (page 10).\n",
      "\n",
      "Sources:\n",
      " • ./pdfs/2023-S1-SE4020-Lecture-02-Introduction.pdf — page 8\n",
      " • ./pdfs/2023-S1-SE4020-Lecture-02-Introduction.pdf — page 1\n",
      " • ./pdfs/2023-S1-SE4020-Lecture-02-Introduction.pdf — page 15\n"
     ]
    }
   ],
   "source": [
    "# 5) Test the chain\n",
    "question = \"what are sets?\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    src = doc.metadata.get(\"source\", \"unknown\")\n",
    "    pg  = doc.metadata.get(\"page\", \"unknown\")\n",
    "    print(f\" • {src} — page {pg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb462d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
